{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"How many times do you want to apply the Perceptron and MLP network ? 100\\n\",\n",
    "      \"('The Perceptron average accuracy of:', 100, 'experiments on raw data is:', 0.7612743823146944)\\n\",\n",
    "      \"('The Perceptron average accuracy of:', 100, 'experiments on preprocessed data is:', 0.763107932379714)\\n\",\n",
    "      \"\\n\",\n",
    "      \"('The MLP average accuracy of:', 100, 'experiments on raw data is:', 0.5035546875)\\n\",\n",
    "      \"('The MLP average accuracy of:', 100, 'experiments on preprocessed data is:', 0.49677042801556426)\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"#1)Your program will first read the above file for the Heart Disease patients.  \\n\",\n",
    "    \"import numpy as np # Use for numpy array\\n\",\n",
    "    \"import csv # for file open\\n\",\n",
    "    \"from Perceptron import PCN # Import my PCN implmentation \\n\",\n",
    "    \"from Helper_Function import sigmoid_act, sigmoid_derivative # Import our activation and  sigmoid functions \\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open(\\\"heart2.csv\\\",'r') as file_h:\\n\",\n",
    "    \"        csvreader = csv.reader(file_h)\\n\",\n",
    "    \"        #Delcare our data set as a list\\n\",\n",
    "    \"        heart_data = []\\n\",\n",
    "    \"        for row in csvreader: # for each row in the csvreader\\n\",\n",
    "    \"            try: # try append val floats for each val in the row on csv reader\\n\",\n",
    "    \"                  heart_data.append([float(val) for val in row])\\n\",\n",
    "    \"            except ValueError: # will will continue if there is non-numeric values such as age, gender...etc\\n\",\n",
    "    \"                  continue\\n\",\n",
    "    \"        #Using numpy we can use this into a array for numpy\\n\",\n",
    "    \"        heart_data = np.array(heart_data) # assigned heart_data into numpy array        \\n\",\n",
    "    \"        #Step 2 \\n\",\n",
    "    \"\\n\",\n",
    "    \"iterations = int(input(\\\"How many times do you want to apply the Perceptron and MLP network ? \\\"))\\n\",\n",
    "    \"count = 0\\n\",\n",
    "    \"#Iniatlize our data sets for raw and preprocessed for PCN\\n\",\n",
    "    \"preprocesses_data_pcn = []\\n\",\n",
    "    \"raw_data_pcn = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"##Iniatlize our data sets for raw and preprocessed for MLP\\n\",\n",
    "    \"preprocesses_data_MLP = []\\n\",\n",
    "    \"raw_data_MLP = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"patience = 10  # Adjust this value as needed\\n\",\n",
    "    \"best_pre_accur_mlp = 0\\n\",\n",
    "    \"epoch_best = 0 # for epoch on early stopping\\n\",\n",
    "    \"\\n\",\n",
    "    \"#Assign our lengths and sizes for MLP\\n\",\n",
    "    \"size_output = 1 #define output level\\n\",\n",
    "    \"size_hidden_layer = 100 #hidden layer size 100\\n\",\n",
    "    \"size_input = len(heart_data[0]) -1 # size of input layer subtact 1 from number of features in heart\\n\",\n",
    "    \"\\n\",\n",
    "    \"#For loop assigned to our iteartions\\n\",\n",
    "    \"for i in range (iterations): \\n\",\n",
    "    \"      \\n\",\n",
    "    \"      #Random shuffle compute\\n\",\n",
    "    \"      np.random.shuffle(heart_data)\\n\",\n",
    "    \"\\n\",\n",
    "    \"      #Separate a quarter of the data for training and the other quarter for testing,\\n\",\n",
    "    \"      # By assigning our boudnaries for testing and trainng data\\n\",\n",
    "    \"      training_size = int (0.25 *len(heart_data)) # Calculatye size of train set quarter\\n\",\n",
    "    \"      training_data = heart_data[:training_size] #Create training set\\n\",\n",
    "    \"      testing_data = heart_data[training_size:] #CReate testing set\\n\",\n",
    "    \"\\n\",\n",
    "    \"      #PCN Initialization\\n\",\n",
    "    \"      pcn = PCN (operations = len(training_data[0])-1) #Initial instance of PCN\\n\",\n",
    "    \"      x_train = training_data[:,:-1] # assign our x on trainng\\n\",\n",
    "    \"      y_train = training_data[:,-1] # assign our y on training\\n\",\n",
    "    \"      pcn.train_compute (x_train,y_train) # train our x and y\\n\",\n",
    "    \"\\n\",\n",
    "    \"      #Apply PCN for preprocessing:\\n\",\n",
    "    \"      #subtact min value by scale of range and sub min value for preprocvessing on training data\\n\",\n",
    "    \"      train_pre_data = (training_data - np.min(training_data, axis = 0) / np.max(training_data, axis = 0) - np.min(training_data, axis = 0)) # Process pre-processed data\\n\",\n",
    "    \"      #Similar for preprocess but for testing \\n\",\n",
    "    \"      test_pro_data = (testing_data - np.min(training_data, axis = 0) / np.max(training_data, axis = 0) - np.min(training_data, axis = 0)) # Process processed test data\\n\",\n",
    "    \"\\n\",\n",
    "    \"      #Grab input feature and label from target from pree processed data training\\n\",\n",
    "    \"      pre_x_train_pcn = train_pre_data [:,:-1]\\n\",\n",
    "    \"      pre_y_train_pcn = train_pre_data [:, -1]\\n\",\n",
    "    \"\\n\",\n",
    "    \"      #Initalize PCN for preprocessed data\\n\",\n",
    "    \"      pcn_pre = PCN(operations = len(train_pre_data[0]) - 1 )\\n\",\n",
    "    \"      pcn_pre.train_compute(pre_x_train_pcn,pre_y_train_pcn) #Train Preprocess data\\n\",\n",
    "    \"      test_pcn_x_pre = test_pro_data[:,:-1] #Grab all input feature from test_pcn_y_pre\\n\",\n",
    "    \"      test_pcn_y_pre = test_pro_data[:,-1] ##Grab all input feature from test_pcn_y_pre\\n\",\n",
    "    \"\\n\",\n",
    "    \"      #Now we calcualte the accuracies of our PCN\\n\",\n",
    "    \"      pcn_acc_raw = np.mean(pcn.Predict(testing_data[:,:-1]) == testing_data[:,-1])\\n\",\n",
    "    \"      pcn_acc_preprocessed = np.mean(pcn_pre.Predict(test_pcn_x_pre) == test_pcn_y_pre) \\n\",\n",
    "    \"\\n\",\n",
    "    \"      #Iniatalize our MLP for data and implementation\\n\",\n",
    "    \"      mlp_size_train = int(0.50 * len(heart_data)) #Calculate size of MLP train whihch is half\\n\",\n",
    "    \"      mlp_size_valid = int(0.25 * len(heart_data)) #Calculate size of MLP train whihch is quarter\\n\",\n",
    "    \"      training_data_mlp = heart_data [:mlp_size_train] #Creqate training data from the rows of mlp_size_train\\n\",\n",
    "    \"      validation_data_mlp = heart_data [mlp_size_train:mlp_size_train +mlp_size_valid] #Create validatio data mlp from rows mlp_size_train to mlp_size_valid \\n\",\n",
    "    \"      testing_data_mlp =  heart_data [mlp_size_train + mlp_size_valid:] #Create testing data for rows mlp_size_train and mlp_size_valid\\n\",\n",
    "    \"\\n\",\n",
    "    \"      # MLP apply normilaization:\\n\",\n",
    "    \"      val_max = np.max(training_data_mlp, axis = 0)  #Calculate the max of training with respect to our axis\\n\",\n",
    "    \"      val_min = np.min(training_data_mlp, axis = 0) #Calcuale the min of training with respetc to our axis\\n\",\n",
    "    \"      \\n\",\n",
    "    \"      for index_feature in range(size_input): # Used for overload issues wince we can compute this part seperately instead in one go\\n\",\n",
    "    \"            min_val = val_min[index_feature]\\n\",\n",
    "    \"            max_val = val_max[index_feature]\\n\",\n",
    "    \"            training_data_mlp[:,index_feature] = (training_data_mlp[:,index_feature] - min_val) / (max_val-min_val)  #Normialize our data by subtact train - min / max -min\\n\",\n",
    "    \"            validation_data_mlp [:,index_feature] = (validation_data_mlp[:,index_feature] - min_val) / (max_val-min_val)  #Normalize our data by sub valid - min  max - min\\n\",\n",
    "    \"            testing_data_mlp [:,index_feature] = (testing_data_mlp[:,index_feature] - min_val) / (max_val-min_val) #Normailize our test - val / max - min\\n\",\n",
    "    \"\\n\",\n",
    "    \"            # training_data_mlp = (training_data_mlp - val_min) / (val_max - val_min) #Normialize our data by subtact train - min / max -min\\n\",\n",
    "    \"            # validation_data_mlp = (validation_data_mlp - val_min) / (val_max - val_min) #Normalize our data by sub valid - min  max - min\\n\",\n",
    "    \"            # testing_data_mlp = (testing_data_mlp - val_min) / (val_max - val_min) #Normailize our test - val / max - min\\n\",\n",
    "    \"\\n\",\n",
    "    \"      #Extract our inputs from x train and y train frrom our training mlp data \\n\",\n",
    "    \"      x_train_mlp, y_train_mlp = training_data_mlp [:,:-1] , training_data_mlp [:,- 1]\\n\",\n",
    "    \"      #Extract our inputs of x an y validation from our validation set\\n\",\n",
    "    \"      x_val_mlp, y_val_mlp = validation_data_mlp[:,:-1], validation_data_mlp[:,-1]\\n\",\n",
    "    \"      #Extract our x and y test from our testing set\\n\",\n",
    "    \"      x_test_mlp, test_y_mlp = testing_data_mlp[:,:-1], testing_data_mlp[:,-1]\\n\",\n",
    "    \"\\n\",\n",
    "    \"      #Assign our weights and bias for MLP hidden inputs and outpust\\n\",\n",
    "    \"      input_hidden_weights = 2 * np.random.rand(size_input,size_hidden_layer) -1\\n\",\n",
    "    \"      output_hidden_weights = 2 * np.random.rand(size_hidden_layer,size_output) -1\\n\",\n",
    "    \"      #Initalzie random weight for input and hidden layer creates matrix of these weights\\n\",\n",
    "    \"\\n\",\n",
    "    \"      \\n\",\n",
    "    \"\\n\",\n",
    "    \"      #Multi-Level Perceptron (MLP) for a fixed number of iterations (say, several hundred or thousand \\n\",\n",
    "    \"      for epoch in range(1000):\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        \\n\",\n",
    "    \"\\n\",\n",
    "    \"        #for our layers (FowardPropagation)\\n\",\n",
    "    \"        input_hidden = np.dot(x_train_mlp,input_hidden_weights) #Calcuate the weighted sum of hidden layer\\n\",\n",
    "    \"        output_hidden = sigmoid_act(input_hidden) # Calculate sigmoid activation function for hidden layer sum output\\n\",\n",
    "    \"        output_input_layer = np.dot(output_hidden,output_hidden_weights) #Calculate weighted sum of inputs of output layer\\n\",\n",
    "    \"        output_output_layer = sigmoid_act(output_input_layer) #Apply sigmoid on output layers\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # (Back Propagation) Note-> d output variable and rest of d are denoted as derivative\\n\",\n",
    "    \"        error_back = y_train_mlp.reshape(-1,1) - output_output_layer #perfom error calculaytion of target table\\n\",\n",
    "    \"        d_output = error_back * sigmoid_derivative(output_output_layer) #Calculate derivative of error by applyoh sigmoid_derivative\\n\",\n",
    "    \"        error_back_hidden = d_output.dot (output_hidden_weights.T) #Calculaye backpropagtion of error on the output layer\\n\",\n",
    "    \"        d_hidden = error_back_hidden * sigmoid_derivative(output_hidden) #Calculate derivative of hidden layer using sigmoid derivative\\n\",\n",
    "    \"\\n\",\n",
    "    \"        #Adjust our bias and weights of MLP Note-> T (is our transpose operation for our martix)\\n\",\n",
    "    \"        output_hidden_weights += output_hidden.T.dot(d_output) #Adjust weights by adding out hidden layer output \\n\",\n",
    "    \"        input_hidden_weights += x_train_mlp.T.dot(d_hidden) #Adjust weights for input layer \\n\",\n",
    "    \"\\n\",\n",
    "    \"      #Perfom calculation of MLP raw data\\n\",\n",
    "    \"        mlp_accuracy_raw = np.mean(np.round(output_output_layer) == y_train_mlp) #Perform our eound on MLP output againts  our y_train_mlp for accuracy \\n\",\n",
    "    \"        #Perform calculation on preprocessed MLP data\\n\",\n",
    "    \"        input_hidden = np.dot(x_test_mlp,input_hidden_weights) # Calculate wighted sum  of input_hidden _weights\\n\",\n",
    "    \"        output_hidden = sigmoid_act(input_hidden) #Calculate our sigmoid with input_hidden\\n\",\n",
    "    \"        output_input_layer = np.dot(output_hidden,output_hidden_weights) #Calculate wighted sum of hidden layers\\n\",\n",
    "    \"        output_output_layer = sigmoid_act(output_input_layer) # Pass again to activation functpon used for our final calculation of accuracy for preprocessed\\n\",\n",
    "    \"        mlp_accuracy_pre = np.mean(np.round(output_output_layer) == test_y_mlp)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        #Early Stopping applied.\\n\",\n",
    "    \"        if mlp_accuracy_pre > best_pre_accur_mlp : # check is current mlp accuracy is our best one\\n\",\n",
    "    \"            best_pre_accur_mlp = mlp_accuracy_pre # if it is assign both accordinly\\n\",\n",
    "    \"            best_epoch = epoch # upate best_epoch to the current\\n\",\n",
    "    \"\\n\",\n",
    "    \"        if  epoch - best_epoch > patience: # else no improvemnnt then we apply early stopping\\n\",\n",
    "    \"            break\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"      #Append our calc to list\\n\",\n",
    "    \"      raw_data_pcn.append(pcn_acc_raw)\\n\",\n",
    "    \"      preprocesses_data_pcn.append(pcn_acc_preprocessed)\\n\",\n",
    "    \"      preprocesses_data_MLP.append(mlp_accuracy_pre)\\n\",\n",
    "    \"      raw_data_MLP.append(mlp_accuracy_raw)\\n\",\n",
    "    \"\\n\",\n",
    "    \"      # count += 1\\n\",\n",
    "    \"      # print (\\\"Iteartions:\\\", count)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"#Finally we can calculate our averae acuracies MLP and PCN\\n\",\n",
    "    \"a_pcn_raw = np.mean(raw_data_pcn)\\n\",\n",
    "    \"a_pcn_pre = np.mean(preprocesses_data_pcn)\\n\",\n",
    "    \"a_mlp_raw = np.mean(raw_data_MLP)\\n\",\n",
    "    \"a_mlp_pre = np.mean(preprocesses_data_MLP)\\n\",\n",
    "    \"\\n\",\n",
    "    \"#Print out average accuracies\\n\",\n",
    "    \"print(\\\"The Perceptron average accuracy of:\\\", iterations, \\\"experiments on raw data is:\\\", a_pcn_raw)\\n\",\n",
    "    \"print(\\\"The Perceptron average accuracy of:\\\", iterations, \\\"experiments on preprocessed data is:\\\", a_pcn_pre)\\n\",\n",
    "    \"print(\\\"\\\")\\n\",\n",
    "    \"print(\\\"The MLP average accuracy of:\\\", iterations, \\\"experiments on raw data is:\\\", a_mlp_raw)\\n\",\n",
    "    \"print(\\\"The MLP average accuracy of:\\\", iterations, \\\"experiments on preprocessed data is:\\\", a_mlp_pre)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"     \\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"      \\n\",\n",
    "    \"      \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 2\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python2\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 2\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython2\",\n",
    "   \"version\": \"2.7.13\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
